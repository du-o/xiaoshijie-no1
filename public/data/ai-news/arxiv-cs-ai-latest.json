{
  "source": "arXiv CS.AI",
  "source_url": "https://export.arxiv.org/rss/cs.AI",
  "fetch_time": "2026-02-17T05:20:34.160Z",
  "articles": [
    {
      "title": "Agentic AI for Commercial Insurance Underwriting with Adversarial Self-Critique",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13213v1 Announce Type: new Abstract: Commercial insurance underwriting is a labor-intensive process that requires manual review of extensive documentation to assess risk and determine policy pricing. While AI offers substantial efficiency improvements, existing solutions lack comprehensive reasoning capabilities and internal mechanisms to ensure reliability within regulated, high-stakes environments. Full automation remains impractical and inadvisable in scenarios where human judgment",
      "url": "https://arxiv.org/abs/2602.13213",
      "category": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "BotzoneBench: Scalable LLM Evaluation via Graded AI Anchors",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13214v1 Announce Type: new Abstract: Large Language Models (LLMs) are increasingly deployed in interactive environments requiring strategic decision-making, yet systematic evaluation of these capabilities remains challenging. Existing benchmarks for LLMs primarily assess static reasoning through isolated tasks and fail to capture dynamic strategic abilities. Recent game-based evaluations employ LLM-vs-LLM tournaments that produce relative rankings dependent on transient model pools, i",
      "url": "https://arxiv.org/abs/2602.13214",
      "category": "cs.AI"
    },
    {
      "title": "When to Think Fast and Slow? AMOR: Entropy-Based Metacognitive Gate for Dynamic SSM-Attention Switching",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13215v1 Announce Type: new Abstract: Transformers allocate uniform computation to every position, regardless of difficulty. State Space Models (SSMs) offer efficient alternatives but struggle with precise information retrieval over a long horizon. Inspired by dual-process theories of cognition (Kahneman, 2011), we propose AMOR (Adaptive Metacognitive Output Router), a hybrid architecture that dynamically engages sparse attention only when an SSM backbone is \"uncertain\"--as measured by",
      "url": "https://arxiv.org/abs/2602.13215",
      "category": "cs.AI"
    },
    {
      "title": "VeRA: Verified Reasoning Data Augmentation at Scale",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13217v1 Announce Type: new Abstract: The main issue with most evaluation schemes today is their \"static\" nature: the same problems are reused repeatedly, allowing for memorization, format exploitation, and eventual saturation. To measure genuine AI progress, we need evaluation that is robust by construction, not by post-hoc detection. In response, we propose VeRA (Verified Reasoning Data Augmentation), a framework that converts benchmark problems into executable specifications, compri",
      "url": "https://arxiv.org/abs/2602.13217",
      "category": "cs.AI"
    },
    {
      "title": "Scaling the Scaling Logic: Agentic Meta-Synthesis of Logic Reasoning",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13218v1 Announce Type: new Abstract: Scaling verifiable training signals remains a key bottleneck for Reinforcement Learning from Verifiable Rewards (RLVR). Logical reasoning is a natural substrate: constraints are formal and answers are programmatically checkable. However, prior synthesis pipelines either depend on expert-written code or operate within fixed templates/skeletons, which limits growth largely to instance-level perturbations. We propose SSLogic, an agentic meta-synthesis",
      "url": "https://arxiv.org/abs/2602.13218",
      "category": [
        "cs.AI",
        "cs.CL",
        "cs.LG",
        "cs.LO"
      ]
    },
    {
      "title": "A Geometric Taxonomy of Hallucinations in LLMs",
      "date": "Tue, 17 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.13224v1 Announce Type: new Abstract: The term \"hallucination\" in large language models conflates distinct phenomena with different geometric signatures in embedding space. We propose a taxonomy identifying three types: unfaithfulness (failure to engage with provided context), confabulation (invention of semantically foreign content), and factual error (incorrect claims within correct conceptual frames). We observe a striking asymmetry. On standard benchmarks where hallucinations are L",
      "url": "https://arxiv.org/abs/2602.13224",
      "category": [
        "cs.AI",
        "cs.CL"
      ]
    }
  ],
  "status": "success",
  "error": null
}