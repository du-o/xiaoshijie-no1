{
  "source": "arXiv CS.AI",
  "source_url": "https://export.arxiv.org/rss/cs.AI",
  "fetch_time": "2026-02-13T12:28:24.696Z",
  "articles": [
    {
      "title": "Explaining AI Without Code: A User Study on Explainable AI",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11159v1 Announce Type: new Abstract: The increasing use of Machine Learning (ML) in sensitive domains such as healthcare, finance, and public policy has raised concerns about the transparency of automated decisions. Explainable AI (XAI) addresses this by clarifying how models generate predictions, yet most methods demand technical expertise, limiting their value for novices. This gap is especially critical in no-code ML platforms, which seek to democratize AI but rarely include explai",
      "url": "https://arxiv.org/abs/2602.11159",
      "category": [
        "cs.AI",
        "cs.HC",
        "cs.LG"
      ]
    },
    {
      "title": "Latent Generative Solvers for Generalizable Long-Term Physics Simulation",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11229v1 Announce Type: new Abstract: We study long-horizon surrogate simulation across heterogeneous PDE systems. We introduce Latent Generative Solvers (LGS), a two-stage framework that (i) maps diverse PDE states into a shared latent physics space with a pretrained VAE, and (ii) learns probabilistic latent dynamics with a Transformer trained by flow matching. Our key mechanism is an uncertainty knob that perturbs latent inputs during training and inference, teaching the solver to co",
      "url": "https://arxiv.org/abs/2602.11229",
      "category": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "On Decision-Valued Maps and Representational Dependence",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11295v1 Announce Type: new Abstract: A computational engine applied to different representations of the same data can produce different discrete outcomes, with some representations preserving the result and others changing it entirely. A decision-valued map records which representations preserve the outcome and which change it, associating each member of a declared representation family with the discrete result it produces. This paper formalizes decision-valued maps and describes Deci",
      "url": "https://arxiv.org/abs/2602.11295",
      "category": [
        "cs.AI",
        "cs.DB"
      ]
    },
    {
      "title": "Voxtral Realtime",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11298v1 Announce Type: new Abstract: We introduce Voxtral Realtime, a natively streaming automatic speech recognition model that matches offline transcription quality at sub-second latency. Unlike approaches that adapt offline models through chunking or sliding windows, Voxtral Realtime is trained end-to-end for streaming, with explicit alignment between audio and text streams. Our architecture builds on the Delayed Streams Modeling framework, introducing a new causal audio encoder an",
      "url": "https://arxiv.org/abs/2602.11298",
      "category": "cs.AI"
    },
    {
      "title": "The PBSAI Governance Ecosystem: A Multi-Agent AI Reference Architecture for Securing Enterprise AI Estates",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11301v1 Announce Type: new Abstract: Enterprises are rapidly deploying large language models, retrieval augmented generation pipelines, and tool using agents into production, often on shared high performance computing clusters and cloud accelerator platforms that also support defensive analytics. These systems increasingly function not as isolated models but as AI estates: socio technical systems spanning models, agents, data pipelines, security tooling, human workflows, and hyperscal",
      "url": "https://arxiv.org/abs/2602.11301",
      "category": [
        "cs.AI",
        "cs.CR"
      ]
    },
    {
      "title": "Dissecting Subjectivity and the \"Ground Truth\" Illusion in Data Annotation",
      "date": "Fri, 13 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.11318v1 Announce Type: new Abstract: In machine learning, \"ground truth\" refers to the assumed correct labels used to train and evaluate models. However, the foundational \"ground truth\" paradigm rests on a positivistic fallacy that treats human disagreement as technical noise rather than a vital sociotechnical signal. This systematic literature review analyzes research published between 2020 and 2025 across seven premier venues: ACL, AIES, CHI, CSCW, EAAMO, FAccT, and NeurIPS, investi",
      "url": "https://arxiv.org/abs/2602.11318",
      "category": [
        "cs.AI",
        "cs.CL",
        "cs.CY"
      ]
    }
  ],
  "status": "success",
  "error": null
}