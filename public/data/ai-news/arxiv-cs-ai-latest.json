{
  "source": "arXiv CS.AI",
  "source_url": "https://export.arxiv.org/rss/cs.AI",
  "fetch_time": "2026-02-12T12:33:01.247Z",
  "articles": [
    {
      "title": "Discovering Differences in Strategic Behavior Between Humans and LLMs",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10324v1 Announce Type: new Abstract: As Large Language Models (LLMs) are increasingly deployed in social and strategic scenarios, it becomes critical to understand where and why their behavior diverges from that of humans. While behavioral game theory (BGT) provides a framework for analyzing behavior, existing models do not fully capture the idiosyncratic behavior of humans or black-box, non-human agents like LLMs. We employ AlphaEvolve, a cutting-edge program discovery tool, to direc",
      "url": "https://arxiv.org/abs/2602.10324",
      "category": [
        "cs.AI",
        "cs.CL",
        "cs.CY",
        "cs.HC"
      ]
    },
    {
      "title": "LiveMedBench: A Contamination-Free Medical Benchmark for LLMs with Automated Rubric Evaluation",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10367v1 Announce Type: new Abstract: The deployment of Large Language Models (LLMs) in high-stakes clinical settings demands rigorous and reliable evaluation. However, existing medical benchmarks remain static, suffering from two critical limitations: (1) data contamination, where test sets inadvertently leak into training corpora, leading to inflated performance estimates; and (2) temporal misalignment, failing to capture the rapid evolution of medical knowledge. Furthermore, current",
      "url": "https://arxiv.org/abs/2602.10367",
      "category": "cs.AI"
    },
    {
      "title": "Found-RL: foundation model-enhanced reinforcement learning for autonomous driving",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10458v1 Announce Type: new Abstract: Reinforcement Learning (RL) has emerged as a dominant paradigm for end-to-end autonomous driving (AD). However, RL suffers from sample inefficiency and a lack of semantic interpretability in complex scenarios. Foundation Models, particularly Vision-Language Models (VLMs), can mitigate this by offering rich, context-aware knowledge, yet their high inference latency hinders deployment in high-frequency RL training loops. To bridge this gap, we presen",
      "url": "https://arxiv.org/abs/2602.10458",
      "category": [
        "cs.AI",
        "cs.LG"
      ]
    },
    {
      "title": "MERIT Feedback Elicits Better Bargaining in LLM Negotiators",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10467v1 Announce Type: new Abstract: Bargaining is often regarded as a logical arena rather than an art or a matter of intuition, yet Large Language Models (LLMs) still struggle to navigate it due to limited strategic depth and difficulty adapting to complex human factors. Current benchmarks rarely capture this limitation. To bridge this gap, we present an utility feedback centric framework. Our contributions are: (i) AgoraBench, a new benchmark spanning nine challenging settings (e.g",
      "url": "https://arxiv.org/abs/2602.10467",
      "category": "cs.AI"
    },
    {
      "title": "Abstraction Generation for Generalized Planning with Pretrained Large Language Models",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10485v1 Announce Type: new Abstract: Qualitative Numerical Planning (QNP) serves as an important abstraction model for generalized planning (GP), which aims to compute general plans that solve multiple instances at once. Recent works show that large language models (LLMs) can function as generalized planners. This work investigates whether LLMs can serve as QNP abstraction generators for GP problems and how to fix abstractions via automated debugging. We propose a prompt protocol: inp",
      "url": "https://arxiv.org/abs/2602.10485",
      "category": "cs.AI"
    },
    {
      "title": "Flow of Spans: Generalizing Language Models to Dynamic Span-Vocabulary via GFlowNets",
      "date": "Thu, 12 Feb 2026 05:00:00 GMT",
      "summary": "arXiv:2602.10583v1 Announce Type: new Abstract: Standard autoregressive language models generate text token-by-token from a fixed vocabulary, inducing a tree-structured state space when viewing token sampling as an action, which limits flexibility and expressiveness. Recent work introduces dynamic vocabulary by sampling retrieved text spans but overlooks that the same sentence can be composed of spans of varying lengths, lacking explicit modeling of the directed acyclic graph (DAG) state space. ",
      "url": "https://arxiv.org/abs/2602.10583",
      "category": "cs.AI"
    }
  ],
  "status": "success",
  "error": null
}